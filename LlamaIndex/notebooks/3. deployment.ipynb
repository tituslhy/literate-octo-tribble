{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nest_asyncio.apply()\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Human in the loop two ways"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. The `HumanResponseEvent` approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.core.workflow.events import InputRequiredEvent, HumanResponseEvent\n",
    "from llama_index.core.chat_engine import SimpleChatEngine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workflow is as simple as it gets! We ask for the username because we only allow a specific user to invoke the workflow. So if the user is not `Titus`, we just simply won't run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class AskLLM(Workflow):\n",
    "    def __init__(self, llm = llm,  *args, **kwargs):\n",
    "        self.user = None\n",
    "        self.approved_users = [\"Titus\"]\n",
    "        self.engine = SimpleChatEngine.from_defaults(llm=llm)\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> InputRequiredEvent | AnswerEvent:\n",
    "        if self.user is None:\n",
    "            print(\"Unidentified user. Verifying credentials...\\n\\n\")\n",
    "            await ctx.set(\"query\", ev.query)\n",
    "            return InputRequiredEvent(prefix=\"Please identify yourself: \")\n",
    "        return AnswerEvent(query=ev.query)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def authenticate(self, ctx: Context, ev: HumanResponseEvent) -> AnswerEvent | StopEvent:\n",
    "        user = ev.response\n",
    "        if user in self.approved_users:\n",
    "            self.user = user\n",
    "            query = await ctx.get(\"query\")\n",
    "            return AnswerEvent(query=str(query))\n",
    "        return StopEvent(result = \"Unauthorized user. Please acquire authorization credentials from administrators.\")\n",
    "    \n",
    "    @step\n",
    "    async def answer(self, ev: AnswerEvent) -> StopEvent:\n",
    "        response = self.engine.chat(ev.query)\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(AskLLM, filename=\"AskLLM.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unidentified user. Verifying credentials...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app1 = AskLLM()\n",
    "\n",
    "handler1 = app1.run(query=\"What is OpenAI?\")\n",
    "\n",
    "## Use a for-loop to handle the `HumanResponseEvent`\n",
    "async for event in handler1.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        response = input(event.prefix)\n",
    "        handler1.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "final_result = await handler1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI is an artificial intelligence research organization that aims to develop and promote friendly AI for the benefit of humanity. Founded in December 2015, OpenAI conducts research in various areas of AI, including machine learning, natural language processing, robotics, and more. The organization is known for creating advanced AI models, such as the GPT (Generative Pre-trained Transformer) series, which includes models like ChatGPT.\n",
       "\n",
       "OpenAI's mission is to ensure that artificial general intelligence (AGI) is aligned with human values and can be used safely and responsibly. The organization emphasizes transparency, collaboration, and ethical considerations in AI development. OpenAI also engages with the broader community through partnerships, publications, and open-source projects to foster a responsible approach to AI technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_result))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. The `StopEvent` approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthEvent(Event):\n",
    "    user: str\n",
    "\n",
    "class AskLLM_2(Workflow):\n",
    "    def __init__(self, llm = llm,  *args, **kwargs):\n",
    "        self.user = None\n",
    "        self.approved_users = [\"Titus\"]\n",
    "        self.engine = SimpleChatEngine.from_defaults(llm=llm)\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> StopEvent | AuthEvent:\n",
    "        if ev.user is None:\n",
    "            return StopEvent(\"Please furnish your user credentials.\")\n",
    "        await ctx.set(\"query\", ev.query)\n",
    "        return AuthEvent(user=ev.user)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def authenticate(self, ctx: Context, ev: AuthEvent) -> AnswerEvent | StopEvent:\n",
    "        user = ev.user\n",
    "        if user in self.approved_users:\n",
    "            self.user = user\n",
    "            query = await ctx.get(\"query\")\n",
    "            return AnswerEvent(query=str(query))\n",
    "        return StopEvent(result = \"Unauthorized user. Please acquire authorization credentials from administrators.\")\n",
    "    \n",
    "    @step\n",
    "    async def answer(self, ev: AnswerEvent) -> StopEvent:\n",
    "        response = self.engine.chat(ev.query)\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AskLLM_all.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(AskLLM_2, filename=\"AskLLM_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please furnish your user credentials."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app2 = AskLLM_2()\n",
    "\n",
    "response = await app2.run(query=\"What is a Large Language Model?\", user=None)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unauthorized user. Please acquire authorization credentials from administrators."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await app2.run(query=\"What is a Large Language Model?\", user=\"Bob\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await app2.run(query=\"What is a Large Language Model?\", user=\"Titus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A Large Language Model (LLM) is a type of artificial intelligence model designed to understand, generate, and manipulate human language. These models are built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data from diverse sources, such as books, articles, websites, and more.\n",
       "\n",
       "Key characteristics of LLMs include:\n",
       "\n",
       "1. **Scale**: LLMs are characterized by their large number of parameters, often in the billions or even trillions. This scale allows them to capture complex patterns and nuances in language.\n",
       "\n",
       "2. **Training**: They are trained using unsupervised or semi-supervised learning methods, where the model learns to predict the next word in a sentence given the previous words. This process helps the model understand grammar, context, and even some level of reasoning.\n",
       "\n",
       "3. **Versatility**: LLMs can perform a wide range of language-related tasks, including text generation, translation, summarization, question answering, and more, often with little to no task-specific training.\n",
       "\n",
       "4. **Contextual Understanding**: They can generate coherent and contextually relevant text based on the input they receive, making them useful for applications like chatbots, content creation, and interactive storytelling.\n",
       "\n",
       "5. **Limitations**: Despite their capabilities, LLMs can produce incorrect or nonsensical answers, may reflect biases present in the training data, and lack true understanding or consciousness.\n",
       "\n",
       "Examples of LLMs include OpenAI's GPT-3 and GPT-4, Google's BERT and T5, and Meta's LLaMA. These models have significantly advanced the field of natural language processing (NLP) and continue to evolve with ongoing research and development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploying Workflows\n",
    "LlamaIndex has many interesting Workflow implementations of research papers! Here's one of my favorites: the `Self-Discovery Workflow`, a \"general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods\".\n",
    "\n",
    "There are two stages for any given task:\n",
    "\n",
    "STAGE-1:\n",
    "\n",
    "a. SELECT: Selects subset of reasoning Modules.\n",
    "\n",
    "b. ADAPT: Adapts selected reasoning modules to the task.\n",
    "\n",
    "c. IMPLEMENT: It gives reasoning structure for the task.\n",
    "\n",
    "STAGE-2: Uses the generated reasoning structure for the task to generate an answer.\n",
    "\n",
    "> Reference: [Paper](https://arxiv.org/abs/2402.03620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "_REASONING_MODULES = [\n",
    "    \"1. How could I devise an experiment to help solve that problem?\",\n",
    "    \"2. Make a list of ideas for solving this problem, and apply them one by one to the problem to see if any progress can be made.\",\n",
    "    \"3. How could I measure progress on this problem?\",\n",
    "    \"4. How can I simplify the problem so that it is easier to solve?\",\n",
    "    \"5. What are the key assumptions underlying this problem?\",\n",
    "    \"6. What are the potential risks and drawbacks of each solution?\",\n",
    "    \"7. What are the alternative perspectives or viewpoints on this problem?\",\n",
    "    \"8. What are the long-term implications of this problem and its solutions?\",\n",
    "    \"9. How can I break down this problem into smaller, more manageable parts?\",\n",
    "    \"10. Critical Thinking: This style involves analyzing the problem from different perspectives, questioning assumptions, and evaluating the evidence or information available. It focuses on logical reasoning, evidence-based decision-making, and identifying potential biases or flaws in thinking.\",\n",
    "    \"11. Try creative thinking, generate innovative and out-of-the-box ideas to solve the problem. Explore unconventional solutions, thinking beyond traditional boundaries, and encouraging imagination and originality.\",\n",
    "    \"12. Seek input and collaboration from others to solve the problem. Emphasize teamwork, open communication, and leveraging the diverse perspectives and expertise of a group to come up with effective solutions.\",\n",
    "    \"13. Use systems thinking: Consider the problem as part of a larger system and understanding the interconnectedness of various elements. Focuses on identifying the underlying causes, feedback loops, and interdependencies that influence the problem, and developing holistic solutions that address the system as a whole.\",\n",
    "    \"14. Use Risk Analysis: Evaluate potential risks, uncertainties, and tradeoffs associated with different solutions or approaches to a problem. Emphasize assessing the potential consequences and likelihood of success or failure, and making informed decisions based on a balanced analysis of risks and benefits.\",\n",
    "    \"15. Use Reflective Thinking: Step back from the problem, take the time for introspection and self-reflection. Examine personal biases, assumptions, and mental models that may influence problem-solving, and being open to learning from past experiences to improve future approaches.\",\n",
    "    \"16. What is the core issue or problem that needs to be addressed?\",\n",
    "    \"17. What are the underlying causes or factors contributing to the problem?\",\n",
    "    \"18. Are there any potential solutions or strategies that have been tried before? If yes, what were the outcomes and lessons learned?\",\n",
    "    \"19. What are the potential obstacles or challenges that might arise in solving this problem?\",\n",
    "    \"20. Are there any relevant data or information that can provide insights into the problem? If yes, what data sources are available, and how can they be analyzed?\",\n",
    "    \"21. Are there any stakeholders or individuals who are directly affected by the problem? What are their perspectives and needs?\",\n",
    "    \"22. What resources (financial, human, technological, etc.) are needed to tackle the problem effectively?\",\n",
    "    \"23. How can progress or success in solving the problem be measured or evaluated?\",\n",
    "    \"24. What indicators or metrics can be used?\",\n",
    "    \"25. Is the problem a technical or practical one that requires a specific expertise or skill set? Or is it more of a conceptual or theoretical problem?\",\n",
    "    \"26. Does the problem involve a physical constraint, such as limited resources, infrastructure, or space?\",\n",
    "    \"27. Is the problem related to human behavior, such as a social, cultural, or psychological issue?\",\n",
    "    \"28. Does the problem involve decision-making or planning, where choices need to be made under uncertainty or with competing objectives?\",\n",
    "    \"29. Is the problem an analytical one that requires data analysis, modeling, or optimization techniques?\",\n",
    "    \"30. Is the problem a design challenge that requires creative solutions and innovation?\",\n",
    "    \"31. Does the problem require addressing systemic or structural issues rather than just individual instances?\",\n",
    "    \"32. Is the problem time-sensitive or urgent, requiring immediate attention and action?\",\n",
    "    \"33. What kinds of solution typically are produced for this kind of problem specification?\",\n",
    "    \"34. Given the problem specification and the current best solution, have a guess about other possible solutions.\"\n",
    "    \"35. Let’s imagine the current best solution is totally wrong, what other ways are there to think about the problem specification?\"\n",
    "    \"36. What is the best way to modify this current best solution, given what you know about these kinds of problem specification?\"\n",
    "    \"37. Ignoring the current best solution, create an entirely new solution to the problem.\"\n",
    "    \"38. Let’s think step by step .\"\n",
    "    \"39. Let’s make a step by step plan and implement it with good notation and explanation.\",\n",
    "]\n",
    "\n",
    "_REASONING_MODULES = \"\\n\".join(_REASONING_MODULES)\n",
    "\n",
    "SELECT_PRMOPT_TEMPLATE = PromptTemplate(\n",
    "    \"Given the task: {task}, which of the following reasoning modules are relevant? Do not elaborate on why.\\n\\n {reasoning_modules}\"\n",
    ")\n",
    "\n",
    "ADAPT_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    \"Without working out the full solution, adapt the following reasoning modules to be specific to our task:\\n{selected_modules}\\n\\nOur task:\\n{task}\"\n",
    ")\n",
    "\n",
    "IMPLEMENT_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    \"Without working out the full solution, create an actionable reasoning structure for the task using these adapted reasoning modules:\\n{adapted_modules}\\n\\nTask Description:\\n{task}\"\n",
    ")\n",
    "\n",
    "REASONING_PROMPT_TEMPLATE = PromptTemplate(\n",
    "    \"Using the following reasoning structure: {reasoning_structure}\\n\\nSolve this task, providing your final answer: {task}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import LLM\n",
    "\n",
    "class JudgeEvent(Event):\n",
    "    task: str\n",
    "    llm: LLM\n",
    "\n",
    "class SetupEvent(Event):\n",
    "    task: str\n",
    "\n",
    "class GetModulesEvent(Event):\n",
    "    \"\"\"Event to get modules.\"\"\"\n",
    "\n",
    "    task: str\n",
    "    modules: str\n",
    "\n",
    "\n",
    "class RefineModulesEvent(Event):\n",
    "    \"\"\"Event to refine modules.\"\"\"\n",
    "\n",
    "    task: str\n",
    "    refined_modules: str\n",
    "\n",
    "\n",
    "class ReasoningStructureEvent(Event):\n",
    "    \"\"\"Event to create reasoning structure.\"\"\"\n",
    "\n",
    "    task: str\n",
    "    reasoning_structure: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfDiscoverWorkflow(Workflow):\n",
    "    \"\"\"Self discover workflow.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: LLM):\n",
    "        self.agent = SimpleChatEngine.from_defaults(llm=llm)\n",
    "    \n",
    "    \n",
    "    @step\n",
    "    async def judge_query(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> StopEvent | SetupEvent:\n",
    "        \"\"\"Event to judge whether a query is well-written and to seek\n",
    "        clarification if the query is not well-written.\"\"\"\n",
    "        \n",
    "        current_chat_history = await ctx.get(\"chat_history\", [])\n",
    "        current_chat_history.append({\"role\": \"user\", \"content\": ev.task})\n",
    "        \n",
    "        response = await self.agent.chat(\n",
    "            f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield good results from a RAG system as-is. If it's good, return 'good', if it's bad, return 'bad', if \n",
    "            it's not a question (for e.g. \"Hi!\", \"Thanks!\"), return \"not a question\".\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\"\n",
    "        )\n",
    "        if response == \"bad\":\n",
    "            final_response = await self.agent.chat(\n",
    "                f\"\"\"\n",
    "                The user has given a poor query. Seek for clarification from the user.\n",
    "                \n",
    "                Here's the user's query: {ev.query}\n",
    "                \"\"\"\n",
    "            )   \n",
    "            current_chat_history.append({\"role\": \"assistant\", \"content\": str(final_response)})\n",
    "            await ctx.set(\"chat_history\", current_chat_history)\n",
    "            return StopEvent(str(final_response)) \n",
    "        \n",
    "        elif response == \"not a question\":\n",
    "            final_response = await self.agent.chat(\n",
    "                f\"\"\"\n",
    "                Here's the user's remark or greeting: {ev.query}\n",
    "                \n",
    "                Respond to the user politely.\n",
    "                \"\"\"\n",
    "            )   \n",
    "            current_chat_history.append({\"role\": \"assistant\", \"content\": str(final_response)})\n",
    "            await ctx.set(\"chat_history\", current_chat_history)\n",
    "            return StopEvent(str(final_response)) \n",
    "        \n",
    "        await ctx.set(\"chat_history\", current_chat_history)\n",
    "        return SetupEvent(task=ev.task)\n",
    "\n",
    "    @step\n",
    "    async def get_modules(\n",
    "        self, ctx: Context, ev: SetupEvent\n",
    "    ) -> GetModulesEvent:\n",
    "        \"\"\"Get modules step.\"\"\"\n",
    "        # get input data, store llm into ctx\n",
    "        task = ev.get(\"task\")\n",
    "\n",
    "        if task is None or llm is None:\n",
    "            raise ValueError(\"'task' and 'llm' arguments are required.\")\n",
    "\n",
    "        # format prompt and get result from LLM\n",
    "        prompt = SELECT_PRMOPT_TEMPLATE.format(\n",
    "            task=task, reasoning_modules=_REASONING_MODULES\n",
    "        )\n",
    "        result = self.llm.complete(prompt)\n",
    "\n",
    "        return GetModulesEvent(task=task, modules=str(result))\n",
    "\n",
    "    @step\n",
    "    async def refine_modules(\n",
    "        self, ctx: Context, ev: GetModulesEvent\n",
    "    ) -> RefineModulesEvent:\n",
    "        \"\"\"Refine modules step.\"\"\"\n",
    "        task = ev.task\n",
    "        modules = ev.modules\n",
    "        llm: LLM = await ctx.get(\"llm\")\n",
    "\n",
    "        # format prompt and get result\n",
    "        prompt = ADAPT_PROMPT_TEMPLATE.format(\n",
    "            task=task, selected_modules=modules\n",
    "        )\n",
    "        result = llm.complete(prompt)\n",
    "\n",
    "        return RefineModulesEvent(task=task, refined_modules=str(result))\n",
    "\n",
    "    @step\n",
    "    async def create_reasoning_structure(\n",
    "        self, ctx: Context, ev: RefineModulesEvent\n",
    "    ) -> ReasoningStructureEvent:\n",
    "        \"\"\"Create reasoning structures step.\"\"\"\n",
    "        task = ev.task\n",
    "        refined_modules = ev.refined_modules\n",
    "        llm: LLM = await ctx.get(\"llm\")\n",
    "\n",
    "        # format prompt, get result\n",
    "        prompt = IMPLEMENT_PROMPT_TEMPLATE.format(\n",
    "            task=task, adapted_modules=refined_modules\n",
    "        )\n",
    "        result = llm.complete(prompt)\n",
    "\n",
    "        return ReasoningStructureEvent(\n",
    "            task=task, reasoning_structure=str(result)\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def get_final_result(\n",
    "        self, ctx: Context, ev: ReasoningStructureEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Gets final result from reasoning structure event.\"\"\"\n",
    "        task = ev.task\n",
    "        reasoning_structure = ev.reasoning_structure\n",
    "        llm: LLM = await ctx.get(\"llm\")\n",
    "\n",
    "        # format prompt, get res\n",
    "        prompt = REASONING_PROMPT_TEMPLATE.format(\n",
    "            task=task, reasoning_structure=reasoning_structure\n",
    "        )\n",
    "        result = llm.complete(prompt)\n",
    "\n",
    "        return StopEvent(result=result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb762c9777d81b920bdc4f2aaae00fd18dcefdd6f60d8272d61e705ce6e5d33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
