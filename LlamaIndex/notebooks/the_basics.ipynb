{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import nest_asyncio\n",
    "import warnings\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MistralAI(model=\"mistral-large-latest\")\n",
    "embed_model = GeminiEmbedding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with an LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the `complete` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue due to a process called Rayleigh scattering. Here's a simple explanation:\n",
       "\n",
       "1. **Light from the Sun**: The Sun emits light at all the visible wavelengths, which is why we see it as white. When this light reaches Earth's atmosphere, it interacts with the gas molecules and tiny particles in the air.\n",
       "\n",
       "2. **Rayleigh Scattering**: Shorter wavelengths of light (like blue and violet) are scattered more by these molecules and particles because they travel in shorter, smaller waves. This is why we see a blue sky most of the time.\n",
       "\n",
       "3. **Why not violet?**: Although violet light is scattered even more than blue light, the sky appears blue, not violet. This is because:\n",
       "   - The Sun emits less violet light compared to blue light.\n",
       "   - Human eyes are more sensitive to blue light than violet light.\n",
       "   - Some violet light gets absorbed by the atmosphere, further tilting the balance towards blue.\n",
       "\n",
       "So, the combination of these factors makes the sky appear blue to us on a clear day. During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.complete(\"Why is the sky blue?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"The sky appears blue due to a process called Rayleigh scattering. Here's a simple explanation:\\n\\n1. **Light from the Sun**: The Sun emits light at all the visible wavelengths, which is why we see it as white. When this light reaches Earth's atmosphere, it interacts with the gas molecules and tiny particles in the air.\\n\\n2. **Rayleigh Scattering**: Shorter wavelengths of light (like blue and violet) are scattered more by these molecules and particles because they travel in shorter, smaller waves. This is why we see a blue sky most of the time.\\n\\n3. **Why not violet?**: Although violet light is scattered even more than blue light, the sky appears blue, not violet. This is because:\\n   - The Sun emits less violet light compared to blue light.\\n   - Human eyes are more sensitive to blue light than violet light.\\n   - Some violet light gets absorbed by the atmosphere, further tilting the balance towards blue.\\n\\nSo, the combination of these factors makes the sky appear blue to us on a clear day. During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\", additional_kwargs={}, raw={'id': 'd62e1f60653745eabcf5c45edbf2cf95', 'object': 'chat.completion', 'model': 'mistral-large-latest', 'usage': UsageInfo(prompt_tokens=9, completion_tokens=301, total_tokens=310), 'created': 1728120934, 'choices': [ChatCompletionChoice(index=0, message=AssistantMessage(content=\"The sky appears blue due to a process called Rayleigh scattering. Here's a simple explanation:\\n\\n1. **Light from the Sun**: The Sun emits light at all the visible wavelengths, which is why we see it as white. When this light reaches Earth's atmosphere, it interacts with the gas molecules and tiny particles in the air.\\n\\n2. **Rayleigh Scattering**: Shorter wavelengths of light (like blue and violet) are scattered more by these molecules and particles because they travel in shorter, smaller waves. This is why we see a blue sky most of the time.\\n\\n3. **Why not violet?**: Although violet light is scattered even more than blue light, the sky appears blue, not violet. This is because:\\n   - The Sun emits less violet light compared to blue light.\\n   - Human eyes are more sensitive to blue light than violet light.\\n   - Some violet light gets absorbed by the atmosphere, further tilting the balance towards blue.\\n\\nSo, the combination of these factors makes the sky appear blue to us on a clear day. During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\", tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response is not a string but a class known as `CompletionResponse` which includes interesting information on the generation. Thankfully, LlamaIndex response classes has a `__str__` method implemented which allows you to apply `str()` to return the text attribute of each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the `chat` function. This requires a list of chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant\"),\n",
    "    ChatMessage(role=\"user\", content=\"Why is the sky blue?\")\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "assistant: The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it collides with the gas molecules and tiny particles in the air. This scattering affects the shorter blue and violet wavelengths of light more than the longer red, orange, and yellow wavelengths. Although violet light is scattered even more than blue light, our eyes are more sensitive to blue light, and the sun emits more blue light than violet. Therefore, we perceive the sky as blue.\n",
       "\n",
       "During sunrise or sunset, the sun is at a low angle, and the light has to pass through more of Earth's atmosphere. This scatters out more of the blue and green light, allowing the reds, oranges, and yellows to reach our eyes, which is why we see those colors during sunrise and sunset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(str(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using `stream_complete` to stream the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.stream_complete(\n",
    "    \"Why is the sky blue?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object llm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict.<locals>.wrapped_gen at 0x3643157e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the response is now a generator object so the `str` function will not work. You will have to print it out chunk by chunk to \"stream\" the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it is made up of all the colors of the visible spectrum: red, orange, yellow, green, blue, and violet. These colors have different wavelengths, with red having the longest and violet having the shortest.\n",
      "\n",
      "When these light waves encounter tiny particles in the atmosphere, like nitrogen and oxygen molecules, they are scattered in different directions. Shorter wavelengths, like blue and violet, are scattered more because they travel in shorter, smaller waves. This is why we see a blue sky most of the time.\n",
      "\n",
      "However, you might wonder why the sky doesn't appear violet, since violet light has an even shorter wavelength than blue light. This is because the sun emits less violet light compared to blue light, and also because human eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tilting the balance towards blue.\n",
      "\n",
      "During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow."
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All LLMs in LlamaIndex have asynchronous capabilities. Which means that you can just add 'a' to any chat functions to invoke an LLM in an asynchronous fashion. \n",
    "> Remember to add `await`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A llama is a large camelid native to South America. Here are some key points about llamas:\n",
       "\n",
       "1. **Scientific Classification**:\n",
       "   - Kingdom: Animalia\n",
       "   - Phylum: Chordata\n",
       "   - Class: Mammalia\n",
       "   - Order: Artiodactyla\n",
       "   - Family: Camelidae\n",
       "   - Genus: Lama\n",
       "   - Species: L. glama\n",
       "\n",
       "2. **Physical Characteristics**:\n",
       "   - Llamas are known for their long neck, banana-shaped ears, and distinctive \"humming\" sound.\n",
       "   - They are large animals, with adults standing about 1.7 to 1.8 meters (5.6 to 5.9 feet) tall at the head, and can weigh between 130 to 200 kg (286 to 440 lbs).\n",
       "   - They have a lifespan of about 15-25 years, though many live into their 30s.\n",
       "\n",
       "3. **Behavior**:\n",
       "   - Llamas are social animals and live in herds.\n",
       "   - They are known for their intelligence and are often used as guard animals for other livestock due to their protective nature.\n",
       "   - Llamas are also used as pack animals due to their strength and endurance.\n",
       "\n",
       "4. **Diet**:\n",
       "   - Llamas are herbivores and primarily graze on grass and ferns.\n",
       "\n",
       "5. **Domestication**:\n",
       "   - Llamas have been domesticated for thousands of years and are still used today for their meat and fiber, as well as pack animals.\n",
       "\n",
       "6. **Relatives**:\n",
       "   - Their closest relatives are alpacas, vicuñas, and guanacos. Camels and dromedaries are also in the same family."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await llm.acomplete(\"What is a Llama?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat history\n",
    "LlamaIndex has a high level abstraction to handle this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"Hello Titus! The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it collides with the gas molecules and tiny particles in the air. This scattering affects the shorter blue and violet wavelengths of light more than the longer red, orange, and yellow wavelengths. Although violet light is scattered even more than blue light, our eyes are more sensitive to blue light, and the sun emits more blue light than violet. As a result, we perceive the sky as blue. During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\", sources=[], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat(\"Hi my name is Titus! Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='Your name is Titus!', sources=[], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat(\"What's my name again?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from typing import List\n",
    "\n",
    "class Exercise(BaseModel):\n",
    "    \"\"\"Generates a workout plan given a focus area\"\"\"\n",
    "    exercise: str\n",
    "    number_of_reps: int\n",
    "    number_of_sets: int\n",
    "\n",
    "class WorkoutPlan(BaseModel):\n",
    "    exercises: List[Exercise]\n",
    "\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    \"\"\"Generate a workout plan given that the user wants to focus on {focus_area}\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Using the `as_structured_llm` approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "workout_plan = (\n",
    "    llm.as_structured_llm(WorkoutPlan)\n",
    "    .complete(prompt_tmpl.format(focus_area=\"biceps\"))\n",
    "    .raw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorkoutPlan(exercises=[Exercise(exercise='Barbell Curl', number_of_reps=10, number_of_sets=3), Exercise(exercise='Hammer Curl', number_of_reps=10, number_of_sets=3), Exercise(exercise='Concentration Curl', number_of_reps=10, number_of_sets=3)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workout_plan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using the `structured_predict` approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorkoutPlan(exercises=[Exercise(exercise='Tricep Dips', number_of_reps=10, number_of_sets=3), Exercise(exercise='Tricep Pushdowns', number_of_reps=12, number_of_sets=3), Exercise(exercise='Overhead Dumbbell Extension', number_of_reps=12, number_of_sets=3)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tricep_plan = llm.structured_predict(WorkoutPlan, prompt_tmpl, focus_area=\"triceps\")\n",
    "tricep_plan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification\n",
    "This is a very common application of llms! We use the `Literal` typing to constraint the LLM's response to within the options specified. We'll assume that you are trying to store a document into a specific folder.\n",
    "\n",
    "> Note that if you add List in front of \"Literal\", the llm will return a list of potential classification results! For e.g. `List[Literal[*class_labels]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DocumentFolder(BaseModel):\n",
    "    \"\"\"Returns only one output folder name given a document name\"\"\"\n",
    "    \n",
    "    folder_name: Literal[\n",
    "        \"finance\",\n",
    "        \"presentations\",\n",
    "        \"sketchbook\",\n",
    "        \"code\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tmpl2 = PromptTemplate(\n",
    "    \"\"\"Help the user sort their documents into folders. This is the document to be sorted: {document_name}\"\"\"\n",
    ")\n",
    "\n",
    "folder = llm.structured_predict(DocumentFolder, prompt_tmpl2, document_name=\"llama_sketch.pix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentFolder(folder_name='sketchbook')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If metadata is available, use this way\n",
    "The previous zero-shot classification only works on a narrow scope of use cases. The class labels must be sufficiently distinct so that the llm can easily return the correct classification. If you have metadata available, there is a better way to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import ToolMetadata\n",
    "\n",
    "choices = [\n",
    "    ToolMetadata(\n",
    "        name = \"presentations\",\n",
    "        description = \"choose this option if the input document has a .pptx or .ppt extension\"\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name = \"python_code\",\n",
    "        description = \"choose this option if the input document has a .py or .ipynb extension\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "selector = LLMSingleSelector.from_defaults(llm=llm)\n",
    "selector_result = selector.select(\n",
    "    choices, query = \"Where should I keep this folder: abc.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSelection(selections=[SingleSelection(index=1, reason='The input document has a .py extension.')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada! The selector returns the index of the selection and even gives the reason for its choice! \n",
    "\n",
    "> Notice that although you imported a `LLMSingleSelector`, the final result is a `MultiSelection` so this approach can do multi-classification out of the box"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex allows for any function to be used as a tool for an LLM. Do check that the LLM is a function-calling LLM first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a*b\n",
    "\n",
    "def mystery(a: int, b: int) -> int:\n",
    "    \"\"\"Mystery function on two integers\"\"\"\n",
    "    return a * b + a + b\n",
    "\n",
    "# Wrap each function into a LlamaIndex FunctionTool\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex also allows for users to specify tools in the form of a class object. This makes it very convenient for users to define sophisticated tools that might share common methods and attributes and inputs (for e.g. stock analysis tools).\n",
    "\n",
    "In this approach, every string in the tool's `spec_functions` attribute is a class method. In this example, this means that the tools defined are `add`, `minus` and `divide`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools.tool_spec.base import BaseToolSpec\n",
    "\n",
    "class ArithmeticTools(BaseToolSpec):\n",
    "    spec_functions = [\n",
    "        \"add\",\n",
    "        \"divide\",\n",
    "        \"minus\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the tool\"\"\"\n",
    "    \n",
    "    def add(self, a: int, b: int) -> int:\n",
    "        \"\"\"Use this tool to add two integers together\"\"\"\n",
    "        return a + b\n",
    "    \n",
    "    def minus(self, a: int, b: int) -> int:\n",
    "        \"\"\"Use this tool to return the difference between two integers\"\"\"\n",
    "        return a - b\n",
    "    \n",
    "    def divide(self, a: int, b: int) -> int:\n",
    "        \"\"\"Use this tool to perform integer division between two integers\"\"\"\n",
    "        return a //b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_tools = ArithmeticTools()\n",
    "tools = ar_tools.to_tool_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex's `BaseToolSpec` based abstractions have a convenience function known as `.to_tool_list()` to return the `spec_functions` methods as a list of LlamaIndex `FunctionTool` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x364485280>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3644851c0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3644c2300>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend our tools list to include the initial tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.extend([mystery_tool, multiply_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x364485280>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3644851c0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3644c2300>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x364439550>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x3644c1310>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, LlamaIndex also has a comprehensive list of tools available on LlamaHub that can be plugged into any function calling LLM.\n",
    "> https://llamahub.ai/?tab=tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we call the tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "47"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    tools = tools,\n",
    "    user_msg=\"What happens when I run the mystery function on 5 and 7\"\n",
    ")\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "It only takes 5 lines to build a sound, basic RAG application that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get our documents\n",
    "> You only need to run this once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-05 17:36:24--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘../data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "../data/paul_graham 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-10-05 17:36:24 (4.95 MB/s) - ‘../data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p '../data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O '../data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lines 1 and 2: Defining your llm and embeddings model\n",
    "We've already done this above but for the sake of completeness, let's do it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = MistralAI(model=\"mistral-large-latest\")\n",
    "embed_model = GeminiEmbedding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line 3: Read files and break texts into document chunks\n",
    "LlamaIndex has a very neat `SimpleDirectoryReader` implementation to do this. Under the hood it can read any file - including audio files (using `whisper` under the hood), presentation and word documents (using Microsoft's pptx and docx python libraries), pdfs, etc. \n",
    "> Note that LlamaIndex also has a LlamaParse tool that allowas for users to reliably read very complex documents: https://www.llamaindex.ai/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    # input_dir = \"../data/paul_graham\" #Use this to read in all documents in the directory\n",
    "    input_files = [\"../data/paul_graham/paul_graham_essay.txt\"] #read in specific files in directory\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`documents` is a list of LlamaIndex's `Document` object where text is chunked. Let's take a look at one of the documents. Each chunk has the main text and some metadata embeddedin the chunks. Let's look at one of these metadata elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '../data/paul_graham/paul_graham_essay.txt',\n",
       " 'file_name': 'paul_graham_essay.txt',\n",
       " 'file_type': 'text/plain',\n",
       " 'file_size': 75042,\n",
       " 'creation_date': '2024-10-05',\n",
       " 'last_modified_date': '2024-10-05'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metadata does not tell a lot, but LlamaIndex has an `IngestionPipeline` abstraction that allows for the llm to be prompted to create metadata based on each chunk. I'll go through this another time for the sake of brevity.\n",
    "> https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line 4: Create embeddings for each chunk and stuff them into a vector store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What vector database are we using? If we do not specify a vector database, we use LlamaIndex's `SimpleVectorStore` as a default!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line 5: Return the index as the query engine\n",
    "Just use the `.as_query_engine()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Titus! Interleaf was a company that, despite having smart people and impressive technology, was eventually overtaken by the rapid advancements in commodity processors, as described by Moore's Law. On the other hand, Viaweb, which was an online store builder, faced challenges in its early stages due to the lack of users, which was evident because they were hosting the stores. To address this, they had to recruit an initial set of users and ensure their stores looked decent before they could launch publicly.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Hi! My name is Titus. What happened at Interleaf and Viaweb?\")\n",
    "display(str(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 lines is all it takes! To save our index, we just need to \"persist\" the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"../data/paul_graham_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load our saved index, just use `load_index_from_storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"../data/paul_graham_index/\")\n",
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Paul Graham wrote code. He mentioned working on a new Lisp called Bel, which he wrote in Arc, and also developed software for online stores and art galleries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine2 = index.as_query_engine(llm=llm)\n",
    "response = query_engine2.query(\"Did Paul Graham write code?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For chat history, just define the index `as_chat_engine`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Paul Graham wrote code. He mentioned working on a new Lisp called Bel, which he wrote in Arc, and also developed software for online stores and art galleries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_engine2 = index.as_chat_engine(llm=llm)\n",
    "response = chat_engine2.query(\"Hi my name is Titus! Did Paul Graham write code?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex's RAG ecosystem is very rich and a single notebook just won't do it. Will cover this more in-depth in the future. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Engine Tool\n",
    "Wrapping a query engine as a tool allows an agent to use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine = query_engine2,\n",
    "    metadata = ToolMetadata(\n",
    "        description=\"Use this tool to answer questions related to Paul Graham\",\n",
    "        name=\"Paul_Graham_Tool\" #remember to use underscores here instead of spaces\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(query_engine_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating an agent\n",
    "\n",
    "LlamaIndex has many agent types (see link) that can address questions using different approaches. The most basic agent forms are the ReAct and FunctionCallingAgent respectively. As agent types get more complex, you will notice an uptick in costs although these agents can better handle harder questions.\n",
    "\n",
    "All agents have chat history out of the box!\n",
    "> https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react import ReActAgent\n",
    "\n",
    "agent = ReActAgent.from_tools(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    verbose = True #To print steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 1593bb84-98c4-44c3-8f24-7de2d158b675. Step input: What is the mystery function's output on 57 and 26\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use the mystery tool to help me answer the question.\n",
      "Action: mystery\n",
      "Action Input: {'a': 57, 'b': 26}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 1565\n",
      "\u001b[0m> Running step 1a02908d-0a79-47f7-bd42-07b0bda9dad3. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The mystery function's output on 57 and 26 is 1565.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the mystery function's output on 57 and 26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The mystery function's output on 57 and 26 is 1565."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step e34f2adb-c3e3-4e37-a286-ec8bf4b27ed4. Step input: Did Paul Graham write code?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: Paul_Graham_Tool\n",
      "Action Input: {'input': 'Did Paul Graham write code?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Yes, Paul Graham wrote code. He mentioned working on a new Lisp called Bel, which took him four years to complete. He also wrote software for generating websites for art galleries and later developed software for building online stores, which eventually became the basis for his company Viaweb.\n",
      "\u001b[0m> Running step a7215d11-4f6f-45e1-8338-241eb3af8590. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Yes, Paul Graham wrote code. He mentioned working on a new Lisp called Bel, which took him four years to complete. He also wrote software for generating websites for art galleries and later developed software for building online stores, which eventually became the basis for his company Viaweb.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, Paul Graham wrote code. He mentioned working on a new Lisp called Bel, which took him four years to complete. He also wrote software for generating websites for art galleries and later developed software for building online stores, which eventually became the basis for his company Viaweb."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = agent.chat(\"Did Paul Graham write code?\")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb762c9777d81b920bdc4f2aaae00fd18dcefdd6f60d8272d61e705ce6e5d33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
