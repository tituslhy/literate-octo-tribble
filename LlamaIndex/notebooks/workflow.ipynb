{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "import nest_asyncio\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nest_asyncio.apply()\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building our first multi-agent system with LlamaIndex Workflows!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Building our RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: llama-trace-v1\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "## Log traces using Llama Trace\n",
    "tracer_provider = register(\n",
    "  project_name=\"llama-trace-v1\",\n",
    "  endpoint=\"https://app.phoenix.arize.com/v1/traces\"\n",
    ")\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Building our RAG Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-09 13:49:00--  https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\n",
      "Resolving s2.q4cdn.com (s2.q4cdn.com)... 139.99.123.118\n",
      "Connecting to s2.q4cdn.com (s2.q4cdn.com)|139.99.123.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 789896 (771K) [application/pdf]\n",
      "Saving to: â€˜apple_2021_10k.pdfâ€™\n",
      "\n",
      "apple_2021_10k.pdf  100%[===================>] 771.38K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-11-09 13:49:00 (13.7 MB/s) - â€˜apple_2021_10k.pdfâ€™ saved [789896/789896]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://s2.q4cdn.com/470004039/files/doc_financials/2021/q4/_10-K-2021-(As-Filed).pdf\" -O apple_2021_10k.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Ingest document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "llm = OpenAI(\"gpt-4o-mini\")\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./apple_2021_10k.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's ingest the Apple 10K pdf file into a hybrid search enabled vector database - Qdrant!\n",
    "Set up Qdrant vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -p 6333:6333 -p 6334:6334 \\\n",
    "#     -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "#     qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: DELETE http://localhost:6333/collections/Apple_10K \"HTTP/1.1 200 OK\"\n",
      "WARNING:llama_index.vector_stores.qdrant.base:Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K/exists \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b723180184d741fc8c65d55c7b613b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14925dd212ac4c71b1638fcc9e568ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K/exists \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab927ab150450bbe4d612d7de915bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd66199882444524b6b67c1d6cd56860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# delete collection if it exists\n",
    "if client.collection_exists(\"Apple_10K\"):\n",
    "    client.delete_collection(\"Apple_10K\")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"Apple_10K\",\n",
    "    client = client,\n",
    "    aclient = aclient,\n",
    "    enable_hybrid = True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/Apple_10K \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/Apple_10K/index?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Apple_10K \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/Apple_10K/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/Apple_10K/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model = embed_model\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab489d369a94513b911a334346da99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a7bb56d9f248f88f2941ba8e939aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227553c958b34c3f847f57992f656232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ee9e926881499dadba0a6ca9bfe969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"Apple_10K\",\n",
    "    client = client,\n",
    "    aclient = aclient,\n",
    "    enable_hybrid = True,\n",
    "    fastembed_sparse_model=\"Qdrant/bm42-all-minilm-l6-v2-attentions\",\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2, sparse_top_k=12, vector_store_query_mode=\"hybrid\", llm = llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"apple_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Apple financials for year 2021. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "\n",
    "apple_agent = FunctionCallingAgent.from_tools(\n",
    "    tools=tools,\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Add our DSPy-LlamaIndex HyDE and Seek tool\n",
    "> Using our Gemini LLM!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "import dspy\n",
    "from dspy import (\n",
    "    Signature,\n",
    "    InputField,\n",
    "    OutputField,\n",
    "    Module,\n",
    "    Predict,\n",
    "    Prediction\n",
    ")\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.evaluate import answer_exact_match, answer_passage_match\n",
    "from dspy import Example\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = Gemini(model=\"models/gemini-1.5-flash\")\n",
    "embed_model2 = GeminiEmbedding()\n",
    "\n",
    "lm = dspy.Google(model=\"gemini-1.5-flash\")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWebPageReader(SimpleWebPageReader):\n",
    "    \"\"\"\n",
    "    Many websites, including Investopedia, require headers like User-Agent to be set in the request to return the correct content.\n",
    "    To fix this, we'll modify the load_data method in the SimpleWebPageReader class to include appropriate headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_data(self, urls):\n",
    "        \"\"\"Edit the headers portion in the load_data method to be able to read .asp files\"\"\"\n",
    "        \n",
    "        if not isinstance(urls, list):\n",
    "            raise ValueError(\"urls must be a list of strings.\")\n",
    "        documents = []\n",
    "        \n",
    "        ## This is the edit\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "        for url in urls:\n",
    "            response = requests.get(url, headers=headers).text\n",
    "            if self.html_to_text:\n",
    "                import html2text\n",
    "                response = html2text.html2text(response)\n",
    "\n",
    "            metadata = None\n",
    "            if self._metadata_fn is not None:\n",
    "                metadata = self._metadata_fn(url)\n",
    "\n",
    "            documents.append(Document(text=response, id_=url, metadata=metadata or {}))\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    \"https://www.investopedia.com/terms/s/stockmarket.asp\",\n",
    "    \"https://www.investopedia.com/ask/answers/difference-between-options-and-futures/\",\n",
    "    \"https://www.investopedia.com/financial-edge/0411/5-essential-things-you-need-to-know-about-every-stock-you-buy.aspx\",\n",
    "    \"https://www.investopedia.com/articles/fundamental/04/063004.asp\",\n",
    "    \"https://www.investopedia.com/terms/t/technicalanalysis.asp\",\n",
    "    \"https://www.investopedia.com/terms/i/ichimoku-cloud.asp\",\n",
    "    \"https://www.investopedia.com/terms/a/aroon.asp\",  \n",
    "    \"https://www.investopedia.com/terms/b/bollingerbands.asp\",\n",
    "    \"https://www.investopedia.com/articles/forex/05/macddiverge.asp\",\n",
    "    \"https://www.investopedia.com/terms/a/accumulationdistribution.asp\",\n",
    "    \"https://www.investopedia.com/terms/s/stochasticoscillator.asp\",\n",
    "    \"https://www.investopedia.com/terms/s/stochrsi.asp\",\n",
    "    \"https://www.investopedia.com/terms/p/price-earningsratio.asp\",\n",
    "    \"https://www.investopedia.com/terms/p/price-to-bookratio.asp\",\n",
    "    \"https://www.investopedia.com/terms/p/price-to-salesratio.asp\",\n",
    "    \"https://www.investopedia.com/terms/q/quickratio.asp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = CustomWebPageReader(\n",
    "    html_to_text=True\n",
    ").load_data(urls=links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "investopedia_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    embed_model=embed_model2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "investopedia_index.storage_context.persist(persist_dir=\"../investopedia_storage\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Load from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "investopedia_storage_context = StorageContext.from_defaults(persist_dir=\"../investopedia_storage\")\n",
    "investopedia_index = load_index_from_storage(investopedia_storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. DSPy Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True, llm = llm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = investopedia_index.as_retriever(embed_model=embed_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    context = dspy.InputField(desc=\"May contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Often between 1-5 sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydeRAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        self.retriever = retriever\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "    \n",
    "    def get_context(self, question):\n",
    "        hyded_query = hyde.run(question)\n",
    "        hyde_nodes = retriever.retrieve(hyded_query.custom_embedding_strs[0]) #retrieve nodes for hypothetical document\n",
    "        nodes = retriever.retrieve(hyded_query.custom_embedding_strs[1]) #retrieve nodes for original question\n",
    "        return \"\\n\\n\".join([n.get_content() for n in hyde_nodes]) + \"\\n\\n\".join([n.get_content() for n in nodes])\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.get_context(question)\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer = prediction.answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v. Test HyDE Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client Google, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    }
   ],
   "source": [
    "engine = HydeRAG()\n",
    "question = \"What is a dividend?\"\n",
    "pred = engine(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vi. Finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "evaluator = SemanticSimilarityEvaluator(\n",
    "    similarity_threshold=0.5, \n",
    "    embed_model=embed_model2\n",
    ")\n",
    "\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    result = evaluator.evaluate(\n",
    "        response = pred.answer,\n",
    "        reference = example.answer\n",
    "    )\n",
    "    return result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "        Example(\n",
    "            question = \"What is the difference between an option and a future?\",\n",
    "            answer = \"\"\"An option gives the buyer the right, but not the obligation, to buy (or sell) an asset at a specific price at any time during the life of the contract.\n",
    "            A futures contract obligates the buyer to purchase a specific asset, and the seller to sell and deliver that asset, at a specific future date\n",
    "            \"\"\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What are the similarities between an option and a future?\",\n",
    "            answer = \"\"\"Futures and options positions may be traded and closed ahead of expiration, but the parties to the futures contracts for commodities are typically obligated to make and accept deliveries on the settlement date.\"\"\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What is an option?\",\n",
    "            answer = \"Options are financial derivatives. An options contract gives an investor the right to buy or sell the underlying instrument at a specific price while the contract is in effect. Investors may choose not to exercise their options. Option holders do not own the underlying shares or enjoy shareholder rights unless they exercise an option to buy stock.\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What are the different options?\",\n",
    "            answer = \"There are only two kinds of options: Call options and put options. A call option confers the right to buy a stock at the strike price before the agreement expires. A put option gives the holder the right to sell a stock at a specific price.\"\n",
    "        ),\n",
    "        Example(\n",
    "            question=\"What's P/E ratio?\",\n",
    "            answer = \"\"\" This ratio is used to measure a company's current share price relative to its per-share earnings. The company can be compared to other, similar corporations so that analysts and investors can determine its relative value. So if a company has a P/E ratio of 20, this means investors are willing to pay $20 for every $1 per earnings. That might seem expensive but not if the company is growing fast. The P/E can be found by comparing the current market price to the cumulative earnings of the last four quarters.\"\"\"\n",
    "        ),\n",
    "        Example(\n",
    "            question=\"What's a dividend?\",\n",
    "            answer = \"\"\"Dividends are like interest in a savings accountâ€”you get paid regardless of the stock price. Dividends are distributions made by a company to its shareholders as a reward from its profits. The amount of the dividend is decided by its board of directors and are generally issued in cash, though it isn't uncommon for some companies to issue dividends in the form of stock shares.\"\"\"\n",
    "        ),\n",
    "        Example(\n",
    "            question=\"What's a balance sheet?\",\n",
    "            answer=\" A balance sheet is a financial statement that reports a company's assets, liabilities and shareholder equity at a specific point in time\"\n",
    "        ),\n",
    "        Example(\n",
    "            question=\"What's a current ratio?\",\n",
    "            answer = \"It's the total current assets divided by total current liabilities, commonly used by analysts to assess the ability of a company to meet its short-term obligations\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What are stocks?\",\n",
    "            answer = \"When you buy a stock or a share, you're getting a piece of that company. Owning shares gives you the right to part of the company's profits, often paid as dividends, and sometimes the right to vote on company matters\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What are REITs?\",\n",
    "            answer=\"Real estate investment trusts (REITs) are companies that own, manage, or finance real estate. Investors can buy shares in them, and they legally must provide 90% of their profits as dividends each year.\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What are brokers?\",\n",
    "            answer = \"Brokers in the stock market play the same role as in insurance and elsewhere, acting as a go-between for investors and the securities markets. They are licensed organizations that buy and sell stocks and other securities for individual and institutional clients.\"\n",
    "        ),\n",
    "        Example(\n",
    "            question = \"What is technical analysis?\",\n",
    "            answer = \"Technical analysis is used to scrutinize the ways supply and demand for a security affect changes in price, volume, and implied volatility. It assumes that past trading activity and price changes of a security can be valuable indicators of the security's future price movements when paired with appropriate investing or trading rules.\"\n",
    "        ),\n",
    "        Example(\n",
    "            question=\"What is the difference between fundamental and technical analysis?\",\n",
    "            answer = \"Fundamental analysis is a method of evaluating securities by attempting to measure the intrinsic value of a stock. Fundamental analysts study everything from the overall economy and industry conditions to the financial condition and management of companies. Technical analysis differs from fundamental analysis in that the stock's price and volume are the only inputs. The core assumption is that all publicly known fundamentals have factored into price; thus, there is no need to pay close attention to them. Technical analysts do not attempt to measure a security's intrinsic value, but instead, use stock charts to identify patterns and trends that suggest how a stock's price will move in the future.\"\n",
    "        )\n",
    "    ]\n",
    "train_examples = [t.with_inputs(\"question\") for t in train_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to deep copy attribute 'retriever' of HydeRAG, falling back to shallow copy or reference copy.\n",
      "WARNING:root:Failed to deep copy attribute 'retriever' of HydeRAG, falling back to shallow copy or reference copy.\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]ERROR:dspy.teleprompt.bootstrap:\u001b[2m2024-11-09T14:42:47.946296Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': 'What is the difference between an option and a future?', 'answer': 'An option gives the buyer the right, but not the obligation, to buy (or sell) an asset at a specific price at any time during the life of the contract.\\n            A futures contract obligates the buyer to purchase a specific asset, and the seller to sell and deliver that asset, at a specific future date\\n            '}) (input_keys={'question'}) with <function validate_context_and_answer at 0x3e35ed300> due to index: 0\n",
      "finish_reason: RECITATION\n",
      ".\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m211\u001b[0m\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:41<01:06,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teleprompter = BootstrapFewShot(\n",
    "    max_labeled_demos=0,\n",
    "    metric=validate_context_and_answer\n",
    ")\n",
    "compiled_dspy_qp = teleprompter.compile(engine, trainset=train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('generate_answer', Predict(GenerateAnswer(context, question -> answer\n",
      "    instructions='Answer questions with short factoid answers.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'May contain relevant facts', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'Often between 1-5 sentences.', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")))]\n"
     ]
    }
   ],
   "source": [
    "compiled_dspy_qp.save(\"../compiled.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vii. Test loading saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading a saved pipeline\n",
    "\n",
    "pipeline = HydeRAG()\n",
    "pipeline.load(\"../compiled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client Google, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference between bollinger bands and the ichimoku cloud?\n",
      "Predicted Answer: The Ichimoku Cloud uses averages based on highs and lows over a period, while Bollinger Bands use a simple moving average (SMA) of closing prices.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the difference between bollinger bands and the ichimoku cloud?\"\n",
    "pred = pipeline(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our LlamaIndex Multi-Agent System using Workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent\n",
    ")\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List, Dict, Optional, Any, Union, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppleEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class InvestopediaEvent(Event):\n",
    "    query: str \n",
    "\n",
    "class CompileEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "    source: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reroute(BaseModel):\n",
    "    reroute: Literal[True, False] = Field(..., description=\"If the answer is satisfactory, return True. Else return False for rerouting\")\n",
    "    improvements: str = Field(..., description=\"Suggestions to improve the response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockSME(Workflow):\n",
    "    def __init__(self, manager_llm = llm, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.llm = llm\n",
    "        \n",
    "        ## Initialize the \"manager\"\n",
    "        self.manager_llm = SimpleChatEngine.from_defaults(llm=manager_llm)\n",
    "        \n",
    "        self.choices = [\n",
    "            ToolMetadata(\n",
    "                name = \"Apple_Agent\",\n",
    "                description = \"\"\"Choose this option to answer any questions related to Apple's financial performance in 2021\n",
    "                including risks, opportunities, and financial statements.\"\"\"\n",
    "            ),\n",
    "            ToolMetadata(\n",
    "                name = \"Definitions_Agent\",\n",
    "                description = \"Choose this option to answer any questions related to investment definitions such as Ichimoku Cloud\"\n",
    "            )\n",
    "        ] \n",
    "        self.router = LLMSingleSelector.from_defaults(llm=self.llm)\n",
    "        self.reroute_prompt_template = PromptTemplate(\n",
    "            \"\"\"Given the query: {query} and the interim answer: {response}, determine if the interim answer satisfies the question.\n",
    "            This does not need to be very strict. As long as the interim answer is able to address the question partially, return True.\"\"\"\n",
    "        )\n",
    "        \n",
    "        ## Initialize apple agent\n",
    "        self.apple_agent = apple_agent\n",
    "        \n",
    "        ## Initialize DSPy definition service\n",
    "        self.definition_service = HydeRAG()\n",
    "        self.definition_service.load(\"../compiled.json\")\n",
    "    \n",
    "    @step\n",
    "    async def route(self, ctx: Context, ev: StartEvent) -> AppleEvent | InvestopediaEvent:\n",
    "        selector_result = await self.router.aselect(self.choices, query = ev.query)\n",
    "        await ctx.set(\"routes\", 0)\n",
    "        for result in selector_result.selections:\n",
    "            if result.index == 0:\n",
    "                ctx.send_event(AppleEvent(query=ev.query))\n",
    "            else:\n",
    "                ctx.send_event(InvestopediaEvent(query=ev.query))\n",
    "            routes = await ctx.get(\"routes\")\n",
    "            routes += 1\n",
    "            await ctx.set(\"routes\", routes)\n",
    "        \n",
    "    @step\n",
    "    async def apple_event(self, ev: AppleEvent) -> CompileEvent:\n",
    "        response = await self.apple_agent.achat(ev.query, history=ev.history)\n",
    "        return CompileEvent(query=ev.query, response = str(response), source = \"apple_agent\")\n",
    "\n",
    "    @step\n",
    "    async def investopedia_event(self, ev: InvestopediaEvent) -> CompileEvent:\n",
    "        response = self.definition_service(ev.query)\n",
    "        return CompileEvent(query=ev.query, response = str(response.answer), source = \"investopedia_agent\")\n",
    "    \n",
    "    @step\n",
    "    async def compile(self, ctx: Context, ev: CompileEvent) -> StopEvent | AppleEvent | InvestopediaEvent:\n",
    "        routes = await ctx.get(\"routes\")\n",
    "        ready = ctx.collect_events(ev, [CompileEvent] * routes)\n",
    "        for i in range(len(ready)):\n",
    "            reroute = await self.llm.astructured_predict(\n",
    "                Reroute, self.reroute_prompt_template, query=ev.query, response = ev.response\n",
    "            )\n",
    "            if reroute.reroute is True:\n",
    "                if ready[i].source == \"apple_agent\":\n",
    "                    return AppleEvent(query = f\"\"\"Here's some feedback to better address the query. \n",
    "                                      Feedback: {reroute.improvements},\n",
    "                                      Query: {ev.query}\"\"\")\n",
    "                else:\n",
    "                    return InvestopediaEvent(query = f\"\"\"Here's some feedback to better address the query. \n",
    "                                            Feedback: {reroute.improvements},\n",
    "                                            Query: {ev.query}\"\"\")\n",
    "        \n",
    "        if routes == 2:\n",
    "            response = await self.manager_llm.achat(\n",
    "                f\"\"\"\n",
    "                 A user has provided a query and 2 different strategies have been used\n",
    "                to answer the query. The query was: {ev.query}\n",
    "\n",
    "                Response 1 ({ready[0].source}): {ready[0].response}\n",
    "                Response 2 ({ready[1].source}): {ready[1].response}\n",
    "\n",
    "                Please compile the responses into a single coherent argument.\n",
    "                \"\"\"\n",
    "            )\n",
    "        else:\n",
    "            response = ready[0].response\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockSME_flow_all.html\n"
     ]
    }
   ],
   "source": [
    "# Draw all\n",
    "draw_all_possible_flows(StockSME, filename=\"StockSME_flow_all.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client Google, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x33a351c10> at 0x3ac04bd80> was created in a different Context\n"
     ]
    }
   ],
   "source": [
    "stockSME = StockSME(timeout=600)\n",
    "response = await stockSME.run(query=\"What is a dividend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A dividend is a distribution of a company's profits to its shareholders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bringing Langgraph into the picture\n",
    "Let's first deploy our langgraph object!\n",
    "\n",
    "`%cd ../../LangChain/notebooks/langgraph_studio/`\n",
    "\n",
    "`!langgraph build -t personal-assistant`\n",
    "\n",
    "`!export LANGSMITH_API_KEY=...`\n",
    "\n",
    "`!export IMAGE_NAME=personal-assistant`\n",
    "\n",
    "`!docker compose up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.pregel.remote import RemoteGraph\n",
    "from langgraph_sdk import get_client, get_sync_client\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "url = \"http://0.0.0.0:8123\"\n",
    "graph_name=\"agents\"\n",
    "client=get_client(url=url)\n",
    "sync_client = get_sync_client(url=url)\n",
    "remote_graph = RemoteGraph(\n",
    "    graph_name, \n",
    "    url=url, \n",
    "    client=client, \n",
    "    sync_client=sync_client\n",
    ")\n",
    "\n",
    "thread = sync_client.threads.create()\n",
    "config = {\n",
    "    \"configurable\": {\"thread_id\": thread[\"thread_id\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await remote_graph.ainvoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"What is the boiling point of water?\")]\n",
    "    },\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The boiling point of water is typically 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure (1 atmosphere or 101.3 kPa). However, this boiling point can change with variations in atmospheric pressure; for example, at higher altitudes where pressure is lower, water boils at a lower temperature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['messages'][-1]['content']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bringing Autogen into the picture\n",
    "Unfortunately we cannot invoke a `GroupChat` here because there is no way for us to terminate the `GroupChat` from within a LlamaIndex Workflow/ LangGraph digraph/ CrewAI Crew. We'll have to stick to a simpler 2-agent setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import (\n",
    "    AssistantAgent,\n",
    "    GroupChat,\n",
    "    GroupChatManager,\n",
    "    UserProxyAgent,\n",
    ")\n",
    "from autogen.cache import Cache\n",
    "\n",
    "config_list=[\n",
    "    {\"model\": \"gpt-4o-mini\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Give the task and send instructions to writer to refine the blog\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"\"\"You are a helpful assistant. Only use the functions you are provided with. \n",
    "    Reply TERMINATE when the task is done\"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"cache_seed\": None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@assistant.register_for_llm(description=\"Gets stock data for ticker of interest\")\n",
    "async def get_stock_data(\n",
    "    ticker: Annotated[str, \"Ticker of interest\"],\n",
    "    end_date: Optional[str] = datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    start_date: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    if start_date is None:\n",
    "        start_date = end_date - timedelta(days=365)\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    # Calculate percentage change over the 12 months\n",
    "    data['Percentage Change'] = ((data['Close'] - data['Close'].iloc[0]) / data['Close'].iloc[0]) * 100\n",
    "\n",
    "    # Identify the highest and lowest stock prices during this period\n",
    "    highest_price = data['Close'].max()\n",
    "    lowest_price = data['Close'].min()\n",
    "\n",
    "    # Calculate average trading volume\n",
    "    average_volume = data['Volume'].mean()\n",
    "\n",
    "    # Prepare results\n",
    "    result = {\n",
    "        'start_price': data['Close'].iloc[0],\n",
    "        'end_price': data['Close'].iloc[-1],\n",
    "        'percentage_change': data['Percentage Change'].iloc[-1],\n",
    "        'highest_price': highest_price,\n",
    "        'lowest_price': lowest_price,\n",
    "        'average_volume': average_volume\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "Write a blogpost about the stock price performance of Apple for the past year.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to Admin):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_YMzmfPL2hmkY4Ased4pAT6nY): get_stock_data *****\u001b[0m\n",
      "Arguments: \n",
      "{\"ticker\":\"AAPL\",\"start_date\":\"2023-11-11\",\"end_date\":\"2024-11-11\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING ASYNC FUNCTION get_stock_data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_YMzmfPL2hmkY4Ased4pAT6nY) *****\u001b[0m\n",
      "{\"start_price\": 184.8000030517578, \"end_price\": 226.9600067138672, \"percentage_change\": 22.813854418769367, \"highest_price\": 236.47999572753906, \"lowest_price\": 165.0, \"average_volume\": 57778281.6}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAssistant\u001b[0m (to Admin):\n",
      "\n",
      "## Apple Inc. Stock Price Performance: A Year in Review\n",
      "\n",
      "As of November 11, 2024, Apple Inc. (AAPL) has shown a remarkable performance in the stock market over the past year. Reflecting a strong and resilient business model, the company's stock price has undergone significant fluctuations yet has ultimately trended upward.\n",
      "\n",
      "### Stock Price Overview\n",
      "\n",
      "At the beginning of the year, on November 11, 2023, Apple's stock was priced at approximately **$184.80**. As of the latest data, the stock price has risen to about **$226.96**. This represents a substantial **22.81%** increase in value, showcasing the company's robust growth trajectory.\n",
      "\n",
      "### Price Fluctuations\n",
      "\n",
      "Over the year, Apple's stock price has had its highs and lows. The highest recorded price during this period was around **$236.48**, while the lowest point reached was **$165.00**. This volatility can be attributed to various factors, including market conditions, global supply chain issues, and the companyâ€™s own earnings reports and product launches.\n",
      "\n",
      "### Trading Volume\n",
      "\n",
      "During this period, Apple has maintained an average trading volume of **57,778,281 shares**. High trading volumes often indicate strong investor interest and can contribute to stock price stability or volatility. This consistent engagement from the market reflects investor confidence in Apple's future prospects.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Overall, Apple's stock performance over the past year highlights its resilience and growth potential in a competitive technology landscape. With a significant increase in stock price and consistent trading volume, Apple continues to be a favorite among investors. As we move into the next year, it will be intriguing to see how the company navigates challenges and opportunities within the technology sector.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with Cache.disk() as cache:\n",
    "    chat_history = await user_proxy.a_initiate_chat(\n",
    "        assistant,\n",
    "        message=\"Write a blogpost about the stock price performance of Apple for the past year.\",\n",
    "        cache=cache,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Apple Inc. Stock Price Performance: A Year in Review\n",
       "\n",
       "As of November 11, 2024, Apple Inc. (AAPL) has shown a remarkable performance in the stock market over the past year. Reflecting a strong and resilient business model, the company's stock price has undergone significant fluctuations yet has ultimately trended upward.\n",
       "\n",
       "### Stock Price Overview\n",
       "\n",
       "At the beginning of the year, on November 11, 2023, Apple's stock was priced at approximately **$184.80**. As of the latest data, the stock price has risen to about **$226.96**. This represents a substantial **22.81%** increase in value, showcasing the company's robust growth trajectory.\n",
       "\n",
       "### Price Fluctuations\n",
       "\n",
       "Over the year, Apple's stock price has had its highs and lows. The highest recorded price during this period was around **$236.48**, while the lowest point reached was **$165.00**. This volatility can be attributed to various factors, including market conditions, global supply chain issues, and the companyâ€™s own earnings reports and product launches.\n",
       "\n",
       "### Trading Volume\n",
       "\n",
       "During this period, Apple has maintained an average trading volume of **57,778,281 shares**. High trading volumes often indicate strong investor interest and can contribute to stock price stability or volatility. This consistent engagement from the market reflects investor confidence in Apple's future prospects.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Overall, Apple's stock performance over the past year highlights its resilience and growth potential in a competitive technology landscape. With a significant increase in stock price and consistent trading volume, Apple continues to be a favorite among investors. As we move into the next year, it will be intriguing to see how the company navigates challenges and opportunities within the technology sector.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chat_history.chat_history[-1]['content']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a LlamaGuard to prevent our service from answering dangerous questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull llama-guard3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "guard_llm = Ollama(\"llama-guard3:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsafe\n",
      "S2\n"
     ]
    }
   ],
   "source": [
    "response = guard_llm.complete(\"How do I steal my friend's car?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe\n"
     ]
    }
   ],
   "source": [
    "response = guard_llm.complete(\"How do I bake cookies?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembling the final workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuardEvent(Event):\n",
    "    answer: str\n",
    "\n",
    "class AppleInvestopediaEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class AnalysisEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class PlanningEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class BadQueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class SearchEvent(Event):\n",
    "    query: str\n",
    "    \n",
    "class ResponseEvent(Event):\n",
    "    query: str\n",
    "    response: str\n",
    "    source: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockAssistant(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        manager_llm = llm,\n",
    "        guard_llm = guard_llm,\n",
    "        config_list = config_list,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.guard_llm = guard_llm\n",
    "        self.manager_llm = manager_llm\n",
    "        self.choices = [\n",
    "            ToolMetadata(\n",
    "                name = \"Apple_and_Investopedia_Agent\",\n",
    "                description = \"\"\"Choose this option to answer any questions related to Apple's financial performance in 2021\n",
    "                including risks, opportunities, and financial statements; or questions relating to investment terminology definitions.\"\"\"\n",
    "            ),\n",
    "            ToolMetadata(\n",
    "                name = \"Analysis_Agent\",\n",
    "                description = \"Choose this option to write blogposts about the financial performance of any stock ticker.\"\n",
    "            ),\n",
    "            ToolMetadata(\n",
    "                name=\"Search_Agent\",\n",
    "                description = \"Choose this option to search the Internet.\"\n",
    "            )\n",
    "        ] \n",
    "        self.router = LLMSingleSelector.from_defaults(llm=self.manager_llm)\n",
    "        self.reroute_prompt_template = PromptTemplate(\n",
    "            \"\"\"Given the query: {query} and the interim answer: {response}, determine if the interim answer satisfies the question.\n",
    "            This does not need to be very strict. As long as the interim answer is able to address the question partially, return True.\"\"\"\n",
    "        )\n",
    "        \n",
    "        ## Langgraph setup\n",
    "        self.remote_graph = RemoteGraph(\n",
    "            graph_name, \n",
    "            url=url, \n",
    "            client=client, \n",
    "            sync_client=sync_client\n",
    "        )\n",
    "        self.thread = sync_client.threads.create()\n",
    "        self.config = {\n",
    "            \"configurable\": {\"thread_id\": thread[\"thread_id\"]}\n",
    "        }\n",
    "        \n",
    "    @step\n",
    "    async def gatekeep(self, ev: StartEvent) -> PlanningEvent | StopEvent:\n",
    "        response = await self.guard_llm.acomplete(ev.query)\n",
    "        if \"unsafe\" in str(response):\n",
    "            return StopEvent(result=\"Your query is inappropriate and I will thus not be answering your question.\")\n",
    "        return PlanningEvent(query = ev.query)\n",
    "    \n",
    "    @step\n",
    "    async def plan_and_route(self, ctx: Context, ev: PlanningEvent) -> StopEvent | AppleInvestopediaEvent | AnalysisEvent | SearchEvent:\n",
    "        evaluation = await self.manager_llm.acomplete(\n",
    "            f\"\"\"\n",
    "            Given a user query, determine if this is likely to yield good results.\n",
    "            \n",
    "            If it's good, return 'good', if it's bad, return 'bad'.\n",
    "            Good queries use a lot of relevant keywords and are detailed. Bad queries are vague or ambiguous.\n",
    "\n",
    "            Here is the query: {ev.query}\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        if evaluation == \"bad\":\n",
    "            final_response = await self.manager_llm.acomplete(\n",
    "                f\"\"\"The user has asked a bad query. Seek for the user's clarification.\"\"\"\n",
    "            )\n",
    "            return StopEvent(str(final_response))\n",
    "        \n",
    "        selector_result = await self.router.aselect(self.choices, query = ev.query)\n",
    "        # print(selector_result)\n",
    "        routes = 0\n",
    "        await ctx.set(\"query\", ev.query)\n",
    "        for result in selector_result.selections:\n",
    "            if result.index == 0:\n",
    "                ctx.send_event(AppleInvestopediaEvent(query=ev.query))\n",
    "            elif result.index == 1:\n",
    "                ctx.send_event(AnalysisEvent(query=ev.query))\n",
    "            else:\n",
    "                ctx.send_event(SearchEvent(query=ev.query))\n",
    "            routes += 1\n",
    "        await ctx.set(\"routes\", routes)\n",
    "    \n",
    "    @step\n",
    "    async def apple_investopedia_event(\n",
    "        self,\n",
    "        ev: AppleInvestopediaEvent,\n",
    "        apple_investopedia_workflow: StockSME\n",
    "    ) -> ResponseEvent:\n",
    "        ## This runs our LlamaIndex StockSME Workflow\n",
    "        response = await apple_investopedia_workflow.run(query=ev.query)\n",
    "        return ResponseEvent(query=ev.query, response=response, source = \"apple_investopedia_agent\")\n",
    "    \n",
    "    @step\n",
    "    async def search_event(\n",
    "        self,\n",
    "        ev: SearchEvent\n",
    "    ) -> ResponseEvent:\n",
    "        ## This is our Langgraph step!\n",
    "        result = await self.remote_graph.ainvoke(\n",
    "            {\n",
    "                \"messages\": [HumanMessage(content=ev.query)]\n",
    "            },\n",
    "            config = self.config\n",
    "        )\n",
    "        # print(result)\n",
    "        # print(result['messages'][1]['content'])\n",
    "        return ResponseEvent(\n",
    "            query = ev.query,\n",
    "            response = result['messages'][1]['content'],\n",
    "            source = \"search_agent\"\n",
    "        )\n",
    "    \n",
    "    @step \n",
    "    async def analysis_event(\n",
    "        self,\n",
    "        ev: AnalysisEvent,\n",
    "    ) -> ResponseEvent:\n",
    "        ## This is our Autogen step!\n",
    "        with Cache.disk() as cache:\n",
    "            chat_history = await user_proxy.a_initiate_chat(\n",
    "                assistant,\n",
    "                message=ev.query,\n",
    "                cache=cache,\n",
    "            )\n",
    "        # print(chat_history)\n",
    "        return ResponseEvent(\n",
    "            query = ev.query,\n",
    "            response = chat_history.chat_history[-1]['content'],\n",
    "            source = \"analysis_agent\"\n",
    "        )\n",
    "    \n",
    "    @step\n",
    "    async def compile(\n",
    "        self,\n",
    "        ctx: Context,\n",
    "        ev: ResponseEvent\n",
    "    ) -> GuardEvent:\n",
    "        routes = await ctx.get(\"routes\")\n",
    "        # print(routes)\n",
    "        ready = ctx.collect_events(ev, [ResponseEvent] * routes)\n",
    "        # print(ready)\n",
    "        for i in range(len(ready)):\n",
    "            reroute = await self.manager_llm.astructured_predict(\n",
    "                Reroute, self.reroute_prompt_template, query=ev.query, response = ev.response\n",
    "            )\n",
    "            if reroute.reroute is True:\n",
    "                if ready[i].source == \"apple_investopedia_agent\":\n",
    "                    return AppleInvestopediaEvent(query = f\"\"\"Here's some feedback to better address the query. \n",
    "                                      Feedback: {reroute.improvements},\n",
    "                                      Query: {ev.query}\"\"\")\n",
    "                elif ready[i].source == \"analysis_agent\":\n",
    "                    return AnalysisEvent(query = f\"\"\"Here's some feedback to better address the query. \n",
    "                                            Feedback: {reroute.improvements},\n",
    "                                            Query: {ev.query}\"\"\")\n",
    "                else:\n",
    "                    return SearchEvent(query = f\"\"\"Here's some feedback to better address the query. \n",
    "                                       Feedback: {reroute.improvements},\n",
    "                                       Query: {ev.query}\"\"\")\n",
    "        \n",
    "        if routes == 2:\n",
    "            response = await self.manager_llm.acomplete(\n",
    "                f\"\"\"\n",
    "                 A user has provided a query and 2 different strategies have been used\n",
    "                to answer the query. The query was: {ev.query}\n",
    "\n",
    "                Response 1 ({ready[0].source}): {ready[0].response}\n",
    "                Response 2 ({ready[1].source}): {ready[1].response}\n",
    "\n",
    "                Please compile the responses into a single coherent argument.\n",
    "                \"\"\"\n",
    "            )\n",
    "        elif routes == 3:\n",
    "            response = await self.manager_llm.acomplete(\n",
    "                f\"\"\"\n",
    "                 A user has provided a query and 3 different strategies have been used\n",
    "                to answer the query. The query was: {ev.query}\n",
    "\n",
    "                Response 1 ({ready[0].source}): {ready[0].response}\n",
    "                Response 2 ({ready[1].source}): {ready[1].response}\n",
    "                Response 3 ({ready[2].source}): {ready[2].response}\n",
    "\n",
    "                Please compile the responses into a single coherent argument.\n",
    "                \"\"\"\n",
    "            )\n",
    "        else:\n",
    "            response = ready[0].response\n",
    "        return GuardEvent(answer = str(response)) \n",
    "\n",
    "    @step\n",
    "    async def check_answer(self, ev: GuardEvent) -> StopEvent:\n",
    "        response = await self.guard_llm.acomplete(ev.answer)\n",
    "        if \"unsafe\" in response:\n",
    "            return StopEvent(\"\"\"The LLM has given an answer that is unsafe. \n",
    "                             We have logged this and reported this to the administrator. \n",
    "                             We apologize. Please ask another question.\"\"\")\n",
    "        return StopEvent(str(ev.answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_assistant = StockAssistant(timeout=3600, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_assistant.add_workflows(apple_investopedia_workflow= StockSME(timeout=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockAssistant.html\n"
     ]
    }
   ],
   "source": [
    "draw_all_possible_flows(StockAssistant, filename=\"StockAssistant.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step gatekeep\n",
      "Step gatekeep produced event PlanningEvent\n",
      "Running step plan_and_route\n",
      "Step plan_and_route produced no event\n",
      "Running step search_event\n",
      "Step search_event produced event ResponseEvent\n",
      "Running step compile\n",
      "Step compile produced event GuardEvent\n",
      "Running step check_answer\n",
      "Step check_answer produced event StopEvent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x17d168180> at 0x3a98fc840> was created in a different Context\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The boiling point of water is typically 100 degrees Celsius (212 degrees Fahrenheit) at standard atmospheric pressure (1 atmosphere or 101.3 kPa). However, the boiling point can vary with changes in atmospheric pressure; for example, at higher altitudes, where the pressure is lower, water boils at a lower temperature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await stock_assistant.run(query=\"What is the boiling point of water?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step gatekeep\n",
      "Step gatekeep produced event PlanningEvent\n",
      "Running step plan_and_route\n",
      "Step plan_and_route produced no event\n",
      "Running step analysis_event\n",
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "Write a blogpost about the stock price performance of Apple for the past year.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to Admin):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_YMzmfPL2hmkY4Ased4pAT6nY): get_stock_data *****\u001b[0m\n",
      "Arguments: \n",
      "{\"ticker\":\"AAPL\",\"start_date\":\"2023-11-11\",\"end_date\":\"2024-11-11\"}\n",
      "\u001b[32m*******************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING ASYNC FUNCTION get_stock_data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_YMzmfPL2hmkY4Ased4pAT6nY) *****\u001b[0m\n",
      "{\"start_price\": 184.8000030517578, \"end_price\": 226.9600067138672, \"percentage_change\": 22.813854418769367, \"highest_price\": 236.47999572753906, \"lowest_price\": 165.0, \"average_volume\": 57778281.6}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to Admin):\n",
      "\n",
      "## Apple Inc. Stock Price Performance: A Year in Review\n",
      "\n",
      "As of November 11, 2024, Apple Inc. (AAPL) has shown a remarkable performance in the stock market over the past year. Reflecting a strong and resilient business model, the company's stock price has undergone significant fluctuations yet has ultimately trended upward.\n",
      "\n",
      "### Stock Price Overview\n",
      "\n",
      "At the beginning of the year, on November 11, 2023, Apple's stock was priced at approximately **$184.80**. As of the latest data, the stock price has risen to about **$226.96**. This represents a substantial **22.81%** increase in value, showcasing the company's robust growth trajectory.\n",
      "\n",
      "### Price Fluctuations\n",
      "\n",
      "Over the year, Apple's stock price has had its highs and lows. The highest recorded price during this period was around **$236.48**, while the lowest point reached was **$165.00**. This volatility can be attributed to various factors, including market conditions, global supply chain issues, and the companyâ€™s own earnings reports and product launches.\n",
      "\n",
      "### Trading Volume\n",
      "\n",
      "During this period, Apple has maintained an average trading volume of **57,778,281 shares**. High trading volumes often indicate strong investor interest and can contribute to stock price stability or volatility. This consistent engagement from the market reflects investor confidence in Apple's future prospects.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Overall, Apple's stock performance over the past year highlights its resilience and growth potential in a competitive technology landscape. With a significant increase in stock price and consistent trading volume, Apple continues to be a favorite among investors. As we move into the next year, it will be intriguing to see how the company navigates challenges and opportunities within the technology sector.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step analysis_event produced event ResponseEvent\n",
      "Running step compile\n",
      "Step compile produced event GuardEvent\n",
      "Running step check_answer\n",
      "Step check_answer produced event StopEvent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x17d168180> at 0x3b5dd1ac0> was created in a different Context\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Apple Inc. Stock Price Performance: A Year in Review\n",
       "\n",
       "As of November 11, 2024, Apple Inc. (AAPL) has shown a remarkable performance in the stock market over the past year. Reflecting a strong and resilient business model, the company's stock price has undergone significant fluctuations yet has ultimately trended upward.\n",
       "\n",
       "### Stock Price Overview\n",
       "\n",
       "At the beginning of the year, on November 11, 2023, Apple's stock was priced at approximately **$184.80**. As of the latest data, the stock price has risen to about **$226.96**. This represents a substantial **22.81%** increase in value, showcasing the company's robust growth trajectory.\n",
       "\n",
       "### Price Fluctuations\n",
       "\n",
       "Over the year, Apple's stock price has had its highs and lows. The highest recorded price during this period was around **$236.48**, while the lowest point reached was **$165.00**. This volatility can be attributed to various factors, including market conditions, global supply chain issues, and the companyâ€™s own earnings reports and product launches.\n",
       "\n",
       "### Trading Volume\n",
       "\n",
       "During this period, Apple has maintained an average trading volume of **57,778,281 shares**. High trading volumes often indicate strong investor interest and can contribute to stock price stability or volatility. This consistent engagement from the market reflects investor confidence in Apple's future prospects.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Overall, Apple's stock performance over the past year highlights its resilience and growth potential in a competitive technology landscape. With a significant increase in stock price and consistent trading volume, Apple continues to be a favorite among investors. As we move into the next year, it will be intriguing to see how the company navigates challenges and opportunities within the technology sector.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await stock_assistant.run(query=\"Write a blogpost about the stock price performance of Apple for the past year.\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step gatekeep\n",
      "Step gatekeep produced event PlanningEvent\n",
      "Running step plan_and_route\n",
      "Step plan_and_route produced no event\n",
      "Running step apple_investopedia_event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated. ***\n",
      " \t\tYou are using the client Google, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n",
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x17d168180> at 0x3b5cbdb80> was created in a different Context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step apple_investopedia_event produced event ResponseEvent\n",
      "Running step compile\n",
      "Step compile produced event GuardEvent\n",
      "Running step check_answer\n",
      "Step check_answer produced event StopEvent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:opentelemetry.context:Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/anaconda3/envs/llamaindex/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x17d168180> at 0x3b5c793c0> was created in a different Context\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A dividend is a distribution of a company's profits to its shareholders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await stock_assistant.run(query=\"What is the definition of a dividend?\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb762c9777d81b920bdc4f2aaae00fd18dcefdd6f60d8272d61e705ce6e5d33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
