{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with an LLM\n",
    "The good thing about langchain is that llms are only `invoke`-ed. There is no need to remember any other method to interact with an LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke on a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today? Let's chat about anything you'd like. 😊"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke on a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant.\"\n",
    "    ),\n",
    "    (\"human\", \"Why is the sky blue?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it is made up of different colors, which are essentially different wavelengths of light. Shorter wavelengths (like blue and violet) are scattered more by the tiny molecules and particles in the atmosphere because they travel in shorter, smaller waves. This is why we perceive the sky as blue most of the time.\\n\\nHowever, you might wonder why the sky doesn't appear violet, since violet light has an even shorter wavelength than blue light. The sky appears blue, not violet, because the sun emits more blue light than violet light, and also because our eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tipping the balance toward blue.\\n\\nDuring sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 17, 'total_tokens': 257, 'completion_tokens': 240}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-a2091759-caab-4c04-922c-dbe640f4e9b2-0', usage_metadata={'input_tokens': 17, 'output_tokens': 240, 'total_tokens': 257})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content=\"The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth\\'s atmosphere, it is made up of different colors, which are essentially different wavelengths of light. Shorter wavelengths (like blue and violet) are scattered more by the tiny molecules and particles in the atmosphere because they travel in shorter, smaller waves. This is why we perceive the sky as blue most of the time.\\\\n\\\\nHowever, you might wonder why the sky doesn\\'t appear violet, since violet light has an even shorter wavelength than blue light. The sky appears blue, not violet, because the sun emits more blue light than violet light, and also because our eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tipping the balance toward blue.\\\\n\\\\nDuring sunrise or sunset, the light has to pass through more of Earth\\'s atmosphere, which scatters more of the blue and green light away, and we\\'re left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\" additional_kwargs={} response_metadata={\\'token_usage\\': {\\'prompt_tokens\\': 17, \\'total_tokens\\': 257, \\'completion_tokens\\': 240}, \\'model\\': \\'mistral-large-latest\\', \\'finish_reason\\': \\'stop\\'} id=\\'run-a2091759-caab-4c04-922c-dbe640f4e9b2-0\\' usage_metadata={\\'input_tokens\\': 17, \\'output_tokens\\': 240, \\'total_tokens\\': 257}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that using the `str` method does not return the content. It just returns the entire output in string format. So we have to access the content attribute directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it is made up of different colors, which are essentially different wavelengths of light. Shorter wavelengths (like blue and violet) are scattered more by the tiny molecules and particles in the atmosphere because they travel in shorter, smaller waves. This is why we perceive the sky as blue most of the time.\n",
       "\n",
       "However, you might wonder why the sky doesn't appear violet, since violet light has an even shorter wavelength than blue light. The sky appears blue, not violet, because the sun emits more blue light than violet light, and also because our eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tipping the balance toward blue.\n",
       "\n",
       "During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the chain abstraction - this is where the magic happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    # Now we instantiate our list of messages\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\") #input is a placeholder\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we defined a placeholder `{input}`. When invoking an llm, you must ensure that there is an argument named `input` or there will be an error. The invocation of the chain is typically done using a dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our chain\n",
    "\n",
    "Langchain allows for users to use the pipe `|` operator to quickly define a chain. This allows for pretty much anything to be chainable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\n",
    "    'input': 'Why is the sky blue?'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it is made up of different colors, which are essentially different wavelengths of light. Shorter wavelengths (like blue and violet) are scattered more by the tiny molecules and particles in the atmosphere because they travel in shorter, smaller waves. This is why we perceive the sky as blue most of the time.\\n\\nHowever, you might wonder why the sky doesn't appear violet, since violet light has an even shorter wavelength than blue light. The sky appears blue, not violet, because the sun emits more blue light than violet light, and also because our eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tipping the balance toward blue.\\n\\nDuring sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 17, 'total_tokens': 257, 'completion_tokens': 240}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-d6ba0c04-1097-4617-9a33-f66493fa67b0-0', usage_metadata={'input_tokens': 17, 'output_tokens': 240, 'total_tokens': 257})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be quite bothersome to continue typing `response.content` to access the string. Thankfully Langchain has an output parser convenience function that we can call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\"input\": \"Why is the sky blue?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue due to a process called Rayleigh scattering. As light from the sun reaches Earth's atmosphere, it is made up of different colors, which are essentially different wavelengths of light. Shorter wavelengths (like blue and violet) are scattered more by the tiny molecules and particles in the atmosphere because they travel in shorter, smaller waves. This is why we perceive the sky as blue most of the time.\n",
       "\n",
       "However, you might wonder why the sky doesn't appear violet, since violet light has an even shorter wavelength than blue light. The sky appears blue, not violet, because the sun emits more blue light than violet light, and also because our eyes are more sensitive to blue light. Additionally, some of the violet light gets absorbed by the atmosphere, further tipping the balance toward blue.\n",
       "\n",
       "During sunrise or sunset, the light has to pass through more of Earth's atmosphere, which scatters more of the blue and green light away, and we're left with the warmer colors of sunrise and sunset, like red, orange, and yellow."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding message history\n",
    "You can continue to append messages to the list of messages, or use Langchain's high level abstraction for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to define our history store and a simple function to return the correct chat history given an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Define our base runnable again\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\") #placeholder\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Wrraping the runnable on top of another runnable\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\" #Must be the same as the placeholder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a session_id within the config\n",
    "config = {\"configurable\": {\"session_id\": \"abc123\"}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content = \"Hi my name is Titus! Why is the sky blue?\")],\n",
    "        \"language\": \"Chinese\"\n",
    "    },\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "你好，Titus！天空之所以是蓝色的，主要是由于大气层中的光散射现象。当太阳光进入地球大气层时，光线会与空气分子和微小颗粒发生碰撞，导致光线发生散射。蓝色光的波长较短，散射的程度比其他颜色的光更强烈，因此我们看到的天空是蓝色的。这种现象被称为瑞利散射。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content = \"What's my name again?\")],\n",
    "        \"language\": \"French\"\n",
    "    },\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Bien sûr, je suis là pour t'aider. Ton nom est Titus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our chat history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi my name is Titus! Why is the sky blue?', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，Titus！天空之所以是蓝色的，主要是由于大气层中的光散射现象。当太阳光进入地球大气层时，光线会与空气分子和微小颗粒发生碰撞，导致光线发生散射。蓝色光的波长较短，散射的程度比其他颜色的光更强烈，因此我们看到的天空是蓝色的。这种现象被称为瑞利散射。', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name again?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Bien sûr, je suis là pour t'aider. Ton nom est Titus.\", additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Exercise(BaseModel):\n",
    "    \"\"\"Generates a workout plan given a focus area\"\"\"\n",
    "    exercise: str\n",
    "    number_of_reps: int\n",
    "    number_of_sets: int\n",
    "\n",
    "class WorkoutPlan(BaseModel):\n",
    "    exercises: List[Exercise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(WorkoutPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = structured_llm.invoke(\"Suggest a workout plan for biceps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorkoutPlan(exercises=[Exercise(exercise='Barbell Curl', number_of_reps=10, number_of_sets=3), Exercise(exercise='Hammer Curl', number_of_reps=10, number_of_sets=3), Exercise(exercise='Concentration Curl', number_of_reps=10, number_of_sets=3)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot classification!\n",
    "Sometimes known as 'routing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DocumentFolder(BaseModel):\n",
    "    \"\"\"Returns only one output folder name given a document name\"\"\"\n",
    "    \n",
    "    folder_name: Literal[\n",
    "        \"finance\",\n",
    "        \"presentations\",\n",
    "        \"sketchbook\",\n",
    "        \"code\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_sorter = llm.with_structured_output(DocumentFolder)\n",
    "response = folder_sorter.invoke(\"Where should I store the file 'llama_sketch.png'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentFolder(folder_name='sketchbook')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With metdata, we can do this via semantic similarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentation = \"choose this option if the input document from the user's query has a .pptx or .ppt extension. Here is the user's query: {query}\"\n",
    "python_code = \"choose this option if the input document from the user's query has a .py or .ipynb extension. Here is the user's query: {query}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we have to define our own router function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "prompt_templates = [presentation, python_code]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    if most_similar == presentation:\n",
    "        return \"presentation\"\n",
    "    return \"python_code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "route_chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = route_chain.invoke(\"Where should I keep this file: abc.py?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_code'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should be starting to get the sense of how flexible Langchain is! Chains can be combined with other chains to form more chains. And a chain can be anything!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling tools\n",
    "Langchain allows for any function to be used as a tool for an LLM. Do check that the LLM is a function-calling LLM first!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To register a function as a tool, simply add the `@tool` decorator on the function of interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a*b\n",
    "\n",
    "@tool\n",
    "def mystery(a: int, b: int) -> int:\n",
    "    \"\"\"Mystery function on two integers\"\"\"\n",
    "    return a * b + a + b\n",
    "\n",
    "@tool\n",
    "def add( a: int, b: int) -> int:\n",
    "    \"\"\"Use this tool to add two integers together\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def minus(a: int, b: int) -> int:\n",
    "    \"\"\"Use this tool to return the difference between two integers\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> int:\n",
    "    \"\"\"Use this tool to perform integer division between two integers\"\"\"\n",
    "    return a //b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, multiply, mystery, minus, divide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "response = llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '0AXkLCfCQ', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}}, {'id': 'NpiIIdDDt', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 11, \"b\": 49}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 390, 'total_tokens': 439, 'completion_tokens': 49}, 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'}, id='run-83123bb6-41a3-4103-a456-2cdb3d63389f-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '0AXkLCfCQ', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'NpiIIdDDt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 49, 'total_tokens': 439})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey there's no response here! What's going on? \n",
    "> Langchain's abstraction means the llm only 'chose' the tool it was going to use without actually executing it. You'll also notice that the tools have `ids`!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the tools that were called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': '0AXkLCfCQ',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'NpiIIdDDt',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the tool outputs and pass the outputs back to the llm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(query)]\n",
    "response = llm_with_tools.invoke(messages)\n",
    "messages.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'HCsQ8cvM0', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}}, {'id': 'UiZ7CKbna', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 11, \"b\": 49}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 390, 'total_tokens': 439, 'completion_tokens': 49}, 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'}, id='run-4ab20826-bc73-4e11-a7a8-c3861dc3d45d-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'HCsQ8cvM0', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'UiZ7CKbna', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 49, 'total_tokens': 439})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='multiply' description='Multiplies two integers and returns the resulting integer' args_schema=<class 'langchain_core.utils.pydantic.multiply'> func=<function multiply at 0x342e8a700>\n",
      "name='add' description='Use this tool to add two integers together' args_schema=<class 'langchain_core.utils.pydantic.add'> func=<function add at 0x342e8a7a0>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\n",
    "        \"add\": add,\n",
    "        \"mystery\": mystery,\n",
    "        \"multiply\": multiply,\n",
    "        \"divide\": divide,\n",
    "        \"minus\": minus,\n",
    "    }[tool_call['name'].lower()]\n",
    "    print(selected_tool)\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'HCsQ8cvM0', 'type': 'function', 'function': {'name': 'multiply', 'arguments': '{\"a\": 3, \"b\": 12}'}}, {'id': 'UiZ7CKbna', 'type': 'function', 'function': {'name': 'add', 'arguments': '{\"a\": 11, \"b\": 49}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 390, 'total_tokens': 439, 'completion_tokens': 49}, 'model': 'mistral-large-latest', 'finish_reason': 'tool_calls'}, id='run-4ab20826-bc73-4e11-a7a8-c3861dc3d45d-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'HCsQ8cvM0', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'UiZ7CKbna', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 49, 'total_tokens': 439}),\n",
       " ToolMessage(content='36', tool_call_id='HCsQ8cvM0'),\n",
       " ToolMessage(content='60', tool_call_id='UiZ7CKbna')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of messages has now been updated with the tool calls and the results of each tool call! Now we just need to send this whole thing to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reply = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of 3 * 12 is 36.\\n\\nThe result of 11 + 49 is 60.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 495, 'total_tokens': 526, 'completion_tokens': 31}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-55f80c86-9ee3-4bbf-9092-d1607e005af5-0', usage_metadata={'input_tokens': 495, 'output_tokens': 31, 'total_tokens': 526})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! now there's actual `content` in the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The result of 3 * 12 is 36.\n",
       "\n",
       "The result of 11 + 49 is 60."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(final_reply.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain does require the user to define quite a lot of things but there is an advantage to this approach. As an example, we can do few-shot prompting by adding examples into the messages list! Few shot-prompting can be quite useful for an llm in learning how to use very complex tools\n",
    "\n",
    "> The examples are copied from https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/#response-reading-tool-calls-from-model-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "examples = [\n",
    "    HumanMessage(\n",
    "        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\"name\": \"multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[{\"name\": \"add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n",
    "    ),\n",
    "    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n",
    "    AIMessage(\n",
    "        \"The product of 317253 and 128472 plus four is 16505054788\",\n",
    "        name=\"example_assistant\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are bad at math but are an expert at using a calculator.\"\"\"\n",
    "\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        *examples,\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='multiply' description='Multiplies two integers and returns the resulting integer' args_schema=<class 'langchain_core.utils.pydantic.multiply'> func=<function multiply at 0x342e8a700>\n",
      "Appended tool output: content='160' tool_call_id='mbk6IkI2A'\n"
     ]
    }
   ],
   "source": [
    "# As before:\n",
    "\n",
    "messages=[HumanMessage(\"What is 119 minus 8 times 20?\")]\n",
    "response = llm_with_tools.invoke(messages)\n",
    "messages.append(response)\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\n",
    "        \"add\": add,\n",
    "        \"mystery\": mystery,\n",
    "        \"multiply\": multiply,\n",
    "        \"divide\": divide,\n",
    "        \"minus\": minus,\n",
    "    }[tool_call['name'].lower()]\n",
    "    print(selected_tool)\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "    print(f\"Appended tool output: {messages[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "Unlike LlamaIndex, langchain does not offer their own vector store abstraction. You must provide a vector store for the llm to access. Let's use Chroma to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../../LlamaIndex/data/paul_graham/paul_graham_essay.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! The entire text file is read as one document. You still must split it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the vector store and ingest the splits\n",
    "> Note: This instantiates an in-memory Chroma vector store instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = embeddings,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once ingested. The first thing we need to do is set the vector store as a retriever. Langchain allows you to do this through the `.as_retriever()` method. We'll also pull a frequently used system prompt for RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Pull prompt (or you can define your own system prompt)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to format documents extracted from retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our RAG chain\n",
    "Now this part is different from the easier \"chains\" we made. We need to specifically tell the chain that the \"context\" for the llm to look at is a small chain (defined to be `retriever | format_docs`). We then also need to have an input key that is a placeholder within the runnable (hence the `RunnablePassthrough`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"Did Paul Graham write code?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Paul Graham wrote code. He mentioned working extensively on a project called Bel and developing a new dialect of Lisp called Arc. He also wrote all of Y Combinator's internal software in Arc."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate an agent\n",
    "Once again there are many different agent types. Let's just call the simplest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `mystery` with `{'a': 57, 'b': 26}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m1565\u001b[0m\u001b[32;1m\u001b[1;3mThe mystery function's output on 57 and 26 is 1565.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What is the mystery function's output on 57 and 26?\",\n",
       " 'output': \"The mystery function's output on 57 and 26 is 1565.\"}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the mystery function's output on 57 and 26?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0b6603876f48329600faf10d97f94fa7f10e717610d69c3559f2fbf4a644e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
